{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe96e905-1e38-4382-9bb1-81746f11aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import time, re, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c192d5-ae75-4e7e-983a-f7a49491d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7879b3c3-5856-44ae-a09a-7e1d25ebc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b6d904-a0b0-45c7-9e30-b1c0281b73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [[\"bells\", \"clocks\"], [\"mugs\", \"cups\"], [\"onions\", \"garlics\"], [\"watches\", \"clocks\"]]\n",
    "template = \"Give me a story about {a} and {b} in {target_language}.\"\n",
    "target_languages = [\"English\", \"Swedish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf32ad4-c4d0-4555-9c19-e04041899a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed functions\n",
    "\n",
    "def make_prompt(pairs, target_languages, template, swap=False):\n",
    "    \"\"\"Return a list of prompts built from pairs and multiple target languages.\"\"\"\n",
    "    prompts = []\n",
    "    for target_language in target_languages:\n",
    "        for item1, item2 in pairs:\n",
    "            a, b = (item2, item1) if swap else (item1, item2)\n",
    "            prompt = template.format(a=a, b=b, target_language=target_language)\n",
    "            prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a4a42b-447b-447e-9035-160e95bc4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038d3c3f-6d61-48a4-8d20-9dea07d375fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_order1 = make_prompt(pairs, target_languages, template, swap=False)\n",
    "prompts_order2 = make_prompt(pairs, target_languages, template, swap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2247f9-e9f5-4482-b4f0-0d78b9d055c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = prompts_order1 + prompts_order2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb56fb7a-3494-42b5-9052-bb2990e96190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Give me a story about bells and clocks in English.',\n",
       " 'Give me a story about mugs and cups in English.',\n",
       " 'Give me a story about onions and garlics in English.',\n",
       " 'Give me a story about watches and clocks in English.',\n",
       " 'Give me a story about bells and clocks in Swedish.',\n",
       " 'Give me a story about mugs and cups in Swedish.',\n",
       " 'Give me a story about onions and garlics in Swedish.',\n",
       " 'Give me a story about watches and clocks in Swedish.',\n",
       " 'Give me a story about clocks and bells in English.',\n",
       " 'Give me a story about cups and mugs in English.',\n",
       " 'Give me a story about garlics and onions in English.',\n",
       " 'Give me a story about clocks and watches in English.',\n",
       " 'Give me a story about clocks and bells in Swedish.',\n",
       " 'Give me a story about cups and mugs in Swedish.',\n",
       " 'Give me a story about garlics and onions in Swedish.',\n",
       " 'Give me a story about clocks and watches in Swedish.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e2ea03-296c-46fe-a415-838f871a6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the model's outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ccffd7f-1364-4511-af27-476fdb420139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_part(resp, msg_idx=0, part_idx=0):\n",
    "    \"\"\"Grab the first text chunk from the Responses API object.\"\"\"\n",
    "    return resp.output[msg_idx].content[part_idx]\n",
    "\n",
    "def build_topk_tables(resp, k=5, msg_idx=0, part_idx=0):\n",
    "    \"\"\"\n",
    "    Returns (wide_df, long_df)\n",
    "\n",
    "    wide_df columns:\n",
    "      i, emitted_token, emitted_p, alt1_token, alt1_p, ..., altk_token, altk_p\n",
    "\n",
    "    long_df columns:\n",
    "      i, rank (0 = emitted), token, p\n",
    "    \"\"\"\n",
    "    part = get_text_part(resp, msg_idx, part_idx)\n",
    "    rows_wide = []\n",
    "    rows_long = []\n",
    "\n",
    "    for i, t in enumerate(part.logprobs):\n",
    "        emitted_tok = t.token\n",
    "        emitted_p   = math.exp(t.logprob)\n",
    "\n",
    "        # Collect candidates from top_logprobs (already the model's best guesses)\n",
    "        alts_raw = getattr(t, \"top_logprobs\", []) or []\n",
    "        # Convert to (token, p) and sort by p desc just in case\n",
    "        alts = [(a.token, math.exp(a.logprob)) for a in alts_raw]\n",
    "        alts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Ensure emitted token appears as rank 0 in long-form, even if it's not in alts\n",
    "        rows_long.append({\"i\": i, \"rank\": 0, \"token\": emitted_tok, \"p\": emitted_p})\n",
    "\n",
    "        # Keep top k alternatives (exclude emitted if it happens to be duplicated in alts)\n",
    "        topk = []\n",
    "        for tok, p in alts:\n",
    "            if tok == emitted_tok and abs(p - emitted_p) < 1e-12:\n",
    "                continue\n",
    "            topk.append((tok, p))\n",
    "            if len(topk) >= k:\n",
    "                break\n",
    "\n",
    "        # Add alternatives to long-form with ranks 1..k\n",
    "        for r, (tok, p) in enumerate(topk, start=1):\n",
    "            rows_long.append({\"i\": i, \"rank\": r, \"token\": tok, \"p\": p})\n",
    "\n",
    "        # Build wide-row\n",
    "        wide_row = {\n",
    "            \"i\": i,\n",
    "            \"emitted_token\": emitted_tok,\n",
    "            \"emitted_p\": emitted_p,\n",
    "        }\n",
    "        for j in range(k):\n",
    "            tokj, pj = (topk[j] if j < len(topk) else (\"\", float(\"nan\")))\n",
    "            wide_row[f\"alt{j+1}_token\"] = tokj\n",
    "            wide_row[f\"alt{j+1}_p\"] = pj\n",
    "        rows_wide.append(wide_row)\n",
    "\n",
    "    wide_df = pd.DataFrame(rows_wide)\n",
    "    long_df = pd.DataFrame(rows_long)\n",
    "    return wide_df, long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "683c917e-20ef-4c0d-b406-0e87305c63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slug(s: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9]+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def id_from_prompt(prompt: str) -> str:\n",
    "    m = re.search(r\"about\\s+(.+?)\\s+and\\s+(.+?)\\s+in\\s+([A-Za-zÅÄÖåäö]+)\", prompt, re.IGNORECASE)\n",
    "    if m:\n",
    "        item1, item2, lang = (slug(m.group(1)), slug(m.group(2)), slug(m.group(3)))\n",
    "        return f\"{item1}_{item2}_{lang}\"\n",
    "    return slug(prompt)[:60]\n",
    "\n",
    "def run_trials(prompts_list, trials=5, model=\"gpt-4o-mini\", top_k=5, out_dir=\"outputs\",\n",
    "               per_request_timeout=20.0, max_output_tokens=120, retries=2, sleep_between=1.0):\n",
    "    \n",
    "    # Create output folder\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Prompts loop\n",
    "    for prompt in prompts_list:\n",
    "        base = id_from_prompt(prompt)\n",
    "        print(f\"\\nPrompt base: {base}\")\n",
    "\n",
    "        # Trials loop\n",
    "        for t in range(1, trials + 1):\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                attempt += 1\n",
    "                try:\n",
    "                    # API request\n",
    "                    resp = client.with_options(timeout=per_request_timeout).responses.create(\n",
    "                        model=model,\n",
    "                        input=prompt,\n",
    "                        max_output_tokens=max_output_tokens,\n",
    "                        top_logprobs=top_k,\n",
    "                        include=[\"message.output_text.logprobs\"]\n",
    "                    )\n",
    "\n",
    "                    # TEXT\n",
    "                    text = resp.output_text\n",
    "\n",
    "                    # LOGS - JSON-serializable\n",
    "                    data = resp.model_dump()\n",
    "                    logprobs = data[\"output\"][0][\"content\"][0][\"logprobs\"]\n",
    "\n",
    "                    # FILE NAMES\n",
    "                    trial_name = f\"{base}_{t:02d}\"\n",
    "                    with open(Path(out_dir) / f\"{trial_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(text)\n",
    "                    with open(Path(out_dir) / f\"{trial_name}_logprobs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(logprobs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "                    print(\"saved:\", trial_name)\n",
    "                    break\n",
    "\n",
    "                except Exception as e:\n",
    "                    if attempt > retries:\n",
    "                        print(f\"[!] Trial {t} failed after {retries} retries: {e}\")\n",
    "                        trial_name = f\"{base}_{t:02d}\"\n",
    "                        with open(Path(out_dir) / f\"{trial_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(f\"[ERROR] {e}\\nPrompt:\\n{prompt}\\n\")\n",
    "                        break\n",
    "                    time.sleep(sleep_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7057ec74-6dd1-4eac-ab42-b5ccb3c0f6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt base: bells_clocks_English\n",
      "saved: bells_clocks_English_01\n"
     ]
    }
   ],
   "source": [
    "# Test trial\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "top_k = 2\n",
    "prompts = all_prompts[0:1]  # one prompt\n",
    "\n",
    "run_trials(prompts, trials=1, model=model, top_k=top_k, out_dir=\"stories\",\n",
    "           per_request_timeout=20.0, max_output_tokens=20, retries=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2128cec-39b2-4504-b07f-f44f9deead64",
   "metadata": {},
   "source": [
    "# Code for actual trials\n",
    "\n",
    "run_trials(\n",
    "    prompts=all_prompts,\n",
    "    trials=5,\n",
    "    model=model,\n",
    "    top_k=top_k,\n",
    "    out_dir=\"stories\",\n",
    "    per_request_timeout=20.0,\n",
    "    retries=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
