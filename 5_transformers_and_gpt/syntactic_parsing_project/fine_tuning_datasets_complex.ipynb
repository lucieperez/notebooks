{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873090e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook has been almost entirely generated using ChatGPT (ChatGPT 4o, November 26, 2024), and the help of Olivier Lauzanne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100f5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c737fe9-81bf-4cc4-985b-84fa5d2209e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file containing these datasets is trial_1 (seed(0))\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9961d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_RATIO = 0.6\n",
    "VALIDATION_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "TOTAL_LINES_SMALL = 200\n",
    "TOTAL_LINES_MEDIUM = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49678da9-35ca-44ff-a4dc-4156482c21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_simple = \"\"\"User input: \"חזון ישעיהו בןאמוץ אשר חזה עליהודה וירושלם בימי עזיהו יותם אחז יחזקיהו מלכי יהודה\"\n",
    "Expected output: [\"חזון ישעיהו בןאמוץ\",\"אשר חזה עליהודה וירושלם בימי עזיהו יותם אחז יחזקיהו מלכי יהודה\"]\"\"\"\n",
    "\n",
    "example_complex = \"\"\"User input: \"יביא יהוה עליך ועלעמך ועלבית אביך ימים אשר לאבאו למיום סוראפרים מעל יהודה את מלך אשור פ\"\n",
    "Expected output: [\"יביא יהוה עליך ועלעמך ועלבית אביך ימים את מלך אשור פ\" ,\"אשר לאבאו למיום\" ,\"סוראפרים מעל יהודה\"]\"\"\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = f\"Identify clauses in this Biblical Hebrew verse and return a JSON list containing the clauses, as shown in the following examples. Do not write anything else than the JSON list in your output. Example 1 {example_simple} Example 2 {example_complex}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003bfc8d-16db-4ff7-8992-410154bed696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a dataset as JSONL\n",
    "\n",
    "def save_as_jsonl(filename, dataset):\n",
    "    \"\"\"Convert a JSON file into JSONL format.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for verse, record in dataset.items():\n",
    "            # Include the verse as part of the JSON object\n",
    "            record_with_verse = {\"verse\": verse, **record}\n",
    "            f.write(json.dumps(record_with_verse, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "\n",
    "def remove_complexity_field(dataset):\n",
    "    \"\"\"Remove the complexity field from a json file.\"\"\"\n",
    "    return {key: {k: v for k, v in value.items() if k != \"complexity\"} for key, value in dataset.items()}\n",
    "\n",
    "\n",
    "def prepare_finetuning_data(system_prompt, dataset, output_json_file, output_jsonl_file):\n",
    "    \"\"\"\n",
    "    Converts a dataset into the format expected for fine-tuning and saves it to JSON and JSONL files.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (dict): The dataset with verses as keys and clauses as values.\n",
    "        output_json_file (str): Path to save the output JSON file.\n",
    "        output_jsonl_file (str): Path to save the output JSONL file.\n",
    "    \"\"\"\n",
    "    # System message content\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT\n",
    "    }\n",
    "\n",
    "    # Create the formatted dataset\n",
    "    formatted_data = []\n",
    "    for verse, content in dataset.items():\n",
    "        # Add system message\n",
    "        messages = [system_message]\n",
    "\n",
    "        # Add user message\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Parse this verse: {verse}\"\n",
    "        }\n",
    "        messages.append(user_message)\n",
    "\n",
    "        # Add assistant message\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": json.dumps(content[\"clauses\"], ensure_ascii=False)\n",
    "        }\n",
    "        messages.append(assistant_message)\n",
    "\n",
    "        # Add to formatted data\n",
    "        formatted_data.append({\"messages\": messages})\n",
    "\n",
    "    # Randomise the order of the lines in formatted data\n",
    "    random.shuffle(formatted_data)\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(output_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(formatted_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Save as JSONL\n",
    "    with open(output_jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in formatted_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808bf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 Load the JSON file\n",
    "file_path = \"data/verses_clauses_dict.json\"  # Replace with your JSON file path\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4602523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract complexities\n",
    "complexities = [item['complexity'] for item in data.values()]\n",
    "\n",
    "# Count occurrences of each complexity type\n",
    "total = len(complexities)\n",
    "complex_count = complexities.count(\"complex\")\n",
    "simple_count = complexities.count(\"simple\")\n",
    "\n",
    "# Complex / simple ratios\n",
    "complex_ratio = complex_count / total\n",
    "simple_ratio = simple_count / total\n",
    "\n",
    "# Calculate percentages\n",
    "complex_percentage = (complex_count / total) * 100\n",
    "simple_percentage = (simple_count / total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53bb9a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Entries: 13084\n",
      "Complex: 1434 (10.96%)\n",
      "Simple: 11650 (89.04%)\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"Total Entries: {total}\")\n",
    "print(f\"Complex: {complex_count} ({complex_percentage:.2f}%)\")\n",
    "print(f\"Simple: {simple_count} ({simple_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b14658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: Generate fine-tuning simple datasets: only keep simple entries\n",
    "complex_items = {k: v for k, v in data.items() if v['complexity'] == 'complex'}\n",
    "simple_items = {k: v for k, v in data.items() if v['complexity'] == 'simple'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c72f3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Shuffle keys for randomness\n",
    "complex_keys = list(complex_items.keys())\n",
    "simple_keys = list(simple_items.keys())\n",
    "random.shuffle(complex_keys)\n",
    "random.shuffle(simple_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a54e00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Distribute `complex` items across datasets\n",
    "complex_ratio_training = TRAINING_RATIO\n",
    "complex_ratio_validation = VALIDATION_RATIO\n",
    "complex_ratio_test = TEST_RATIO\n",
    "\n",
    "# Total number of `complex` entries\n",
    "num_complex = len(complex_keys)\n",
    "\n",
    "# Calculate how many `complex` items go into each dataset\n",
    "num_training_complex = int(num_complex * complex_ratio_training)\n",
    "num_validation_complex = int(num_complex * complex_ratio_validation)\n",
    "num_test_complex = num_complex - num_training_complex - num_validation_complex  # Remaining for test\n",
    "\n",
    "# Split `complex` items\n",
    "complex_training = complex_keys[:num_training_complex]\n",
    "remaining_complex = complex_keys[num_training_complex:]\n",
    "complex_validation = remaining_complex[:num_validation_complex]\n",
    "complex_test = remaining_complex[num_validation_complex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c7ffb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split `simple` items across datasets with adjustable ratios\n",
    "training_ratio = TRAINING_RATIO \n",
    "validation_ratio = VALIDATION_RATIO  \n",
    "test_ratio = TEST_RATIO \n",
    "\n",
    "# Total number of `simple` entries\n",
    "num_simple = len(simple_keys)\n",
    "\n",
    "# Calculate how many `simple` items go into each dataset\n",
    "num_training_simple = int(num_simple * training_ratio)\n",
    "num_validation_simple = int(num_simple * validation_ratio)\n",
    "num_test_simple = num_simple - num_training_simple - num_validation_simple  # Remaining for test\n",
    "\n",
    "# Split `simple` items\n",
    "simple_training = simple_keys[:num_training_simple]\n",
    "remaining_simple = simple_keys[num_training_simple:]\n",
    "validation_simple = remaining_simple[:num_validation_simple]\n",
    "test_simple = remaining_simple[num_validation_simple:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e75d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Combine splits to form datasets\n",
    "training = {**{k: complex_items[k] for k in complex_training}}\n",
    "validation = {**{k: complex_items[k] for k in complex_validation}}\n",
    "test = {**{k: complex_items[k] for k in complex_test}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5270484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 860 entries\n",
      "Validation dataset: 286 entries\n",
      "Test dataset: 288 entries\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Output statistics and save datasets\n",
    "print(f\"Training dataset: {len(training)} entries\")\n",
    "print(f\"Validation dataset: {len(validation)} entries\")\n",
    "print(f\"Test dataset: {len(test)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cafb8edb-c9c5-4f51-ba5d-97ff9030341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: Generate fine-tuning datasets: Separate `complex` and `simple` entries\n",
    "complex_training_items = {k: v for k, v in training.items() if v['complexity'] == 'complex'}\n",
    "simple_training_items = {k: v for k, v in training.items() if v['complexity'] == 'simple'}\n",
    "\n",
    "# Step 3: Shuffle keys for randomness\n",
    "complex_training_keys = list(complex_training_items.keys())\n",
    "simple_training_keys = list(simple_training_items.keys())\n",
    "random.shuffle(complex_training_keys)\n",
    "random.shuffle(simple_training_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3761b5af-1396-430e-b18a-54896151f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: Generate fine-tuning datasets: Separate `complex` and `simple` entries\n",
    "complex_validation_items = {k: v for k, v in validation.items() if v['complexity'] == 'complex'}\n",
    "simple_validation_items = {k: v for k, v in validation.items() if v['complexity'] == 'simple'}\n",
    "\n",
    "# Step 3: Shuffle keys for randomness\n",
    "complex_validation_keys = list(complex_validation_items.keys())\n",
    "simple_validation_keys = list(simple_validation_items.keys())\n",
    "random.shuffle(complex_validation_keys)\n",
    "random.shuffle(simple_validation_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a8576c-1fc7-4eff-aced-ddab4a834b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: Generate fine-tuning datasets: Separate `complex` and `simple` entries\n",
    "complex_test_items = {k: v for k, v in test.items() if v['complexity'] == 'complex'}\n",
    "simple_test_items = {k: v for k, v in test.items() if v['complexity'] == 'simple'}\n",
    "\n",
    "# Step 3: Shuffle keys for randomness\n",
    "complex_test_keys = list(complex_test_items.keys())\n",
    "simple_test_keys = list(simple_test_items.keys())\n",
    "random.shuffle(complex_test_keys)\n",
    "random.shuffle(simple_test_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a20d7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex in training: 860\n",
      "Complex in validation: 286\n",
      "Complex in test: 288\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of complex and simple elements between datasets\n",
    "training_complex = sum(1 for v in training.values() if v.get(\"complexity\") == \"complex\")\n",
    "validation_complex = sum(1 for v in validation.values() if v.get(\"complexity\") == \"complex\")\n",
    "test_complex = sum(1 for v in test.values() if v.get(\"complexity\") == \"complex\")\n",
    "\n",
    "print(f\"Complex in training: {training_complex}\")\n",
    "print(f\"Complex in validation: {validation_complex}\")\n",
    "print(f\"Complex in test: {test_complex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45cb47eb-c064-4f79-a693-f6590e4ccc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save datasets to JSON files with complexity\n",
    "with open(\"data/fine_tuning_datasets/trial_4/training_with_complexity_complex.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(training, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/trial_4/validation_with_complexity_complex.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(validation, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/trial_4/test_with_complexity_complex.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac9c47bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the complexity field\n",
    "# Save dataset without complexity\n",
    "\n",
    "training_no_complexity = remove_complexity_field(training)\n",
    "validation_no_complexity = remove_complexity_field(validation)\n",
    "test_no_complexity = remove_complexity_field(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84a4a86a-f12d-445d-a81e-3c2ae4a73de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 860 entries\n",
      "Validation dataset: 286 entries\n",
      "Test dataset: 288 entries\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset: {len(training_no_complexity)} entries\")\n",
    "print(f\"Validation dataset: {len(validation_no_complexity)} entries\")\n",
    "print(f\"Test dataset: {len(test_no_complexity)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea951e02-916c-4138-b7ad-eeae9cbd0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to json and jsonl files\n",
    "\n",
    "# training_large\n",
    "prepare_finetuning_data(SYSTEM_PROMPT, training_no_complexity, \"data/fine_tuning_datasets/trial_4/training_complex_large.json\", \"data/fine_tuning_datasets/trial_4/training_complex_large.jsonl\")\n",
    "\n",
    "# validation_large\n",
    "prepare_finetuning_data(SYSTEM_PROMPT, validation_no_complexity, \"data/fine_tuning_datasets/trial_4/validation_complex_large.json\", \"data/fine_tuning_datasets/trial_4/validation_complex_large.jsonl\")\n",
    "\n",
    "# test_large\n",
    "prepare_finetuning_data(SYSTEM_PROMPT, test_no_complexity, \"data/fine_tuning_datasets/trial_4/test_complex_large.json\", \"data/fine_tuning_datasets/trial_4/test_complex_large.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff08d25",
   "metadata": {},
   "source": [
    "#### Dataset 200 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a2a7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many lines for `complex` and `simple` in each small dataset\n",
    "training_complex_count = int(TOTAL_LINES_SMALL * 100)\n",
    "validation_complex_count = int(TOTAL_LINES_SMALL * 100)\n",
    "test_complex_count = int(TOTAL_LINES_SMALL * 100)\n",
    "\n",
    "training_simple_count = TOTAL_LINES_SMALL - training_complex_count\n",
    "validation_simple_count = TOTAL_LINES_SMALL - validation_complex_count\n",
    "test_simple_count = TOTAL_LINES_SMALL - test_complex_count\n",
    "\n",
    "# Step 2: Select `complex` entries for small datasets\n",
    "training_complex_small = complex_training_keys[:training_complex_count]\n",
    "validation_complex_small = complex_validation_keys[:validation_complex_count]\n",
    "test_complex_small = complex_test_keys[:test_complex_count]\n",
    "\n",
    "# Step 3: Select `simple` entries for small datasets\n",
    "training_simple_small = simple_training_keys[:training_simple_count]\n",
    "validation_simple_small = simple_validation_keys[:validation_simple_count]\n",
    "test_simple_small = simple_test_keys[:test_simple_count]\n",
    "\n",
    "# Step 4: Combine `complex` and `simple` entries for small datasets\n",
    "training_small = {**{k: complex_items[k] for k in training_complex_small}}\n",
    "validation_small = {**{k: complex_items[k] for k in validation_complex_small}}\n",
    "test_small = {**{k: complex_items[k] for k in test_complex_small}}\n",
    "\n",
    "# Step 5: Verify and save the small datasets\n",
    "\n",
    "training_small_no_complexity = remove_complexity_field(training_small)\n",
    "validation_small_no_complexity = remove_complexity_field(validation_small)\n",
    "test_small_no_complexity = remove_complexity_field(test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4eb5ee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 860 entries\n",
      "Validation dataset: 286 entries\n",
      "Test dataset: 288 entries\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset: {len(training_small)} entries\")\n",
    "print(f\"Validation dataset: {len(validation_small)} entries\")\n",
    "print(f\"Test dataset: {len(test_small)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46e11eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the small datasets to JSON files (with complexity)\n",
    "with open(\"data/fine_tuning_datasets/trial_2/training_small_with_complexity_simple.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(training_small, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/trial_2/validation_small_with_complexity_simple.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(validation_small, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/trial_2/test_small_with_complexity_simple.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test_small, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a15d6818-11d7-4c7a-bf34-de1a62b65747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to json and jsonl files for fine-tuning\n",
    "\n",
    "# training_large\n",
    "prepare_finetuning_data(SYSTEM_PROMPT, training_small_no_complexity, \"data/fine_tuning_datasets/trial_2/training_simple_small.json\", \"data/fine_tuning_datasets/trial_2/training_simple_small.jsonl\")\n",
    "\n",
    "# validation_large\n",
    "prepare_finetuning_data(SYSTEM_PROMPT, validation_small_no_complexity, \"data/fine_tuning_datasets/trial_2/validation_simple_small.json\", \"data/fine_tuning_datasets/trial_2/validation_simple_small.jsonl\")\n",
    "\n",
    "# test_large\n",
    "prepare_finetuning_data(SYSTEM_PROMPT, test_small_no_complexity, \"data/fine_tuning_datasets/trial_2/test_simple_small.json\", \"data/fine_tuning_datasets/trial_2/test_simple_small.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041d371",
   "metadata": {},
   "source": [
    "#### Dataset 1000 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84eaea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many lines for `complex` and `simple` in each medium dataset\n",
    "training_complex_count = int(TOTAL_LINES_MEDIUM * 100)\n",
    "validation_complex_count = int(TOTAL_LINES_MEDIUM * 100)\n",
    "test_complex_count = int(TOTAL_LINES_MEDIUM * 100)\n",
    "\n",
    "training_simple_count = TOTAL_LINES_MEDIUM - training_complex_count\n",
    "validation_simple_count = TOTAL_LINES_MEDIUM - validation_complex_count\n",
    "test_simple_count = TOTAL_LINES_MEDIUM - test_complex_count\n",
    "\n",
    "\n",
    "# Step 2: Select `complex` entries for medium datasets\n",
    "training_complex_medium = complex_training_keys[:training_complex_count]\n",
    "validation_complex_medium = complex_validation_keys[:validation_complex_count]\n",
    "test_complex_medium = complex_test_keys[:test_complex_count]\n",
    "\n",
    "# Step 3: Select `simple` entries for medium datasets\n",
    "training_simple_medium = simple_training_keys[:training_simple_count]\n",
    "validation_simple_medium = simple_validation_keys[:validation_simple_count]\n",
    "test_simple_medium = simple_test_keys[:test_simple_count]\n",
    "\n",
    "# Step 4: Combine `complex` and `simple` entries for medium datasets\n",
    "training_medium = {**{k: complex_items[k] for k in training_complex_medium},\n",
    "                  **{k: simple_items[k] for k in training_simple_medium}}\n",
    "validation_medium = {**{k: complex_items[k] for k in validation_complex_medium},\n",
    "                    **{k: simple_items[k] for k in validation_simple_medium}}\n",
    "test_medium = {**{k: complex_items[k] for k in test_complex_medium},\n",
    "              **{k: simple_items[k] for k in test_simple_medium}}\n",
    "\n",
    "# Step 5: Verify and save the medium datasets\n",
    "\n",
    "training_medium_no_complexity = remove_complexity_field(training_medium)\n",
    "validation_medium_no_complexity = remove_complexity_field(validation_medium)\n",
    "test_medium_no_complexity = remove_complexity_field(test_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfd2dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 860 entries\n",
      "Validation dataset: 286 entries\n",
      "Test dataset: 288 entries\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset: {len(training_medium)} entries\")\n",
    "print(f\"Validation dataset: {len(validation_medium)} entries\")\n",
    "print(f\"Test dataset: {len(test_medium)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b0ee8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the medium datasets to JSON files (with complexity)\n",
    "with open(\"data/fine_tuning_datasets/trial_2/training_medium_with_complexity_simple.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(training_medium, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/trial_2/validation_medium_with_complexity_simple.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(validation_medium, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/trial_2/test_medium_with_complexity_simple.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test_medium, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7199f94-4694-4ed3-9d79-7d0315bb81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to json and jsonl files for fine-tuning\n",
    "\n",
    "# training_large\n",
    "prepare_finetuning_data(SYSTEM_PROMPT, training_medium_no_complexity, \"data/fine_tuning_datasets/trial_2/training_simple_medium.json\", \"data/fine_tuning_datasets/trial_2/training_simple_medium.jsonl\")\n",
    "\n",
    "# validation_large\n",
    "prepare_finetuning_data(SYSTEM_PROMPT, validation_medium_no_complexity, \"data/fine_tuning_datasets/trial_2/validation_simple_medium.json\", \"data/fine_tuning_datasets/trial_2/validation_simple_medium.jsonl\")\n",
    "\n",
    "# test_large\n",
    "prepare_finetuning_data(SYSTEM_PROMPT, test_medium_no_complexity, \"data/fine_tuning_datasets/trial_2/test_simple_medium.json\", \"data/fine_tuning_datasets/trial_2/test_simple_medium.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85c0fb4f-8690-4e54-920f-79d819e24562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check intersections between variables\n",
    "\n",
    "datasets = {\n",
    "    \"training_small\": training_small,\n",
    "    \"training_medium\": training_medium,\n",
    "    \"training\": training,\n",
    "    \"validation_small\": validation_small,\n",
    "    \"validation_medium\": validation_medium,\n",
    "    \"validation\": validation,\n",
    "    \"test_small\": test_small,\n",
    "    \"test_medium\": test_medium,\n",
    "    \"test\": test,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5755592-f3ed-435a-978e-b5274e5d7aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between training_small and training_medium: 200\n",
      "Overlap between training_small and training: 200\n",
      "Overlap between training_medium and training_small: 200\n",
      "Overlap between training_medium and training: 1000\n",
      "Overlap between training and training_small: 200\n",
      "Overlap between training and training_medium: 1000\n",
      "Overlap between validation_small and validation_medium: 200\n",
      "Overlap between validation_small and validation: 200\n",
      "Overlap between validation_medium and validation_small: 200\n",
      "Overlap between validation_medium and validation: 1000\n",
      "Overlap between validation and validation_small: 200\n",
      "Overlap between validation and validation_medium: 1000\n",
      "Overlap between test_small and test_medium: 200\n",
      "Overlap between test_small and test: 200\n",
      "Overlap between test_medium and test_small: 200\n",
      "Overlap between test_medium and test: 1000\n",
      "Overlap between test and test_small: 200\n",
      "Overlap between test and test_medium: 1000\n"
     ]
    }
   ],
   "source": [
    "for name1, dataset1 in datasets.items():\n",
    "    for name2, dataset2 in datasets.items():\n",
    "        if name1 == name2:\n",
    "            continue\n",
    "        keys1 = set(dataset1.keys())\n",
    "        keys2 = set(dataset2.keys())\n",
    "        intersection = len(keys1 & keys2)\n",
    "        if intersection != 0:\n",
    "            print(f\"Overlap between {name1} and {name2}: {intersection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "811612ef-3ca8-4281-a485-e37aa782592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the amount of complex and simple in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5962febc-1aac-4a28-b668-10e814ca40d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_small\n",
      "Total Entries: 200\n",
      "Complex: 0 (0.00%)\n",
      "Simple: 200 (100.00%)\n",
      "\n",
      "training_medium\n",
      "Total Entries: 1000\n",
      "Complex: 0 (0.00%)\n",
      "Simple: 1000 (100.00%)\n",
      "\n",
      "training\n",
      "Total Entries: 6990\n",
      "Complex: 0 (0.00%)\n",
      "Simple: 6990 (100.00%)\n",
      "\n",
      "validation_small\n",
      "Total Entries: 200\n",
      "Complex: 0 (0.00%)\n",
      "Simple: 200 (100.00%)\n",
      "\n",
      "validation_medium\n",
      "Total Entries: 1000\n",
      "Complex: 0 (0.00%)\n",
      "Simple: 1000 (100.00%)\n",
      "\n",
      "validation\n",
      "Total Entries: 2330\n",
      "Complex: 0 (0.00%)\n",
      "Simple: 2330 (100.00%)\n",
      "\n",
      "test_small\n",
      "Total Entries: 200\n",
      "Complex: 0 (0.00%)\n",
      "Simple: 200 (100.00%)\n",
      "\n",
      "test_medium\n",
      "Total Entries: 1000\n",
      "Complex: 0 (0.00%)\n",
      "Simple: 1000 (100.00%)\n",
      "\n",
      "test\n",
      "Total Entries: 2330\n",
      "Complex: 0 (0.00%)\n",
      "Simple: 2330 (100.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in datasets.items():\n",
    "    complexities = [item['complexity'] for item in dataset.values()]\n",
    "\n",
    "    # Count occurrences of each complexity type\n",
    "    total = len(complexities)\n",
    "    complex_count = complexities.count(\"complex\")\n",
    "    simple_count = complexities.count(\"simple\")\n",
    "    \n",
    "    # Calculate percentages\n",
    "    complex_percentage = (complex_count / total) * 100\n",
    "    simple_percentage = (simple_count / total) * 100\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{name}\")\n",
    "    print(f\"Total Entries: {total}\")\n",
    "    print(f\"Complex: {complex_count} ({complex_percentage:.2f}%)\")\n",
    "    print(f\"Simple: {simple_count} ({simple_percentage:.2f}%)\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007b62f-2893-43da-9dd1-8cf9770b27dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
