{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "873090e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook has been almost entirely generated using ChatGPT (ChatGPT 4o, November 26, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "100f5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9961d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_RATIO = 0.6\n",
    "VALIDATION_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "TOTAL_LINES_SMALL = 200\n",
    "TOTAL_LINES_MEDIUM = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "808bf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 Load the JSON file\n",
    "file_path = \"data/verses_clauses_dict.json\"  # Replace with your JSON file path\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4602523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract complexities\n",
    "complexities = [item['complexity'] for item in data.values()]\n",
    "\n",
    "# Count occurrences of each complexity type\n",
    "total = len(complexities)\n",
    "complex_count = complexities.count(\"complex\")\n",
    "simple_count = complexities.count(\"simple\")\n",
    "\n",
    "# Calculate percentages\n",
    "complex_percentage = (complex_count / total) * 100\n",
    "simple_percentage = (simple_count / total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "53bb9a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Entries: 13084\n",
      "Complex: 1434 (10.96%)\n",
      "Simple: 11650 (89.04%)\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"Total Entries: {total}\")\n",
    "print(f\"Complex: {complex_count} ({complex_percentage:.2f}%)\")\n",
    "print(f\"Simple: {simple_count} ({simple_percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b14658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: Generate fine-tuning datasets: Separate `complex` and `simple` entries\n",
    "complex_items = {k: v for k, v in data.items() if v['complexity'] == 'complex'}\n",
    "simple_items = {k: v for k, v in data.items() if v['complexity'] == 'simple'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c72f3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Shuffle keys for randomness\n",
    "complex_keys = list(complex_items.keys())\n",
    "simple_keys = list(simple_items.keys())\n",
    "random.shuffle(complex_keys)\n",
    "random.shuffle(simple_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a54e00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Distribute `complex` items across datasets\n",
    "complex_ratio_training = TRAINING_RATIO\n",
    "complex_ratio_validation = VALIDATION_RATIO\n",
    "complex_ratio_test = TEST_RATIO\n",
    "\n",
    "# Total number of `complex` entries\n",
    "num_complex = len(complex_keys)\n",
    "\n",
    "# Calculate how many `complex` items go into each dataset\n",
    "num_training_complex = int(num_complex * complex_ratio_training)\n",
    "num_validation_complex = int(num_complex * complex_ratio_validation)\n",
    "num_test_complex = num_complex - num_training_complex - num_validation_complex  # Remaining for test\n",
    "\n",
    "# Split `complex` items\n",
    "complex_training = complex_keys[:num_training_complex]\n",
    "remaining_complex = complex_keys[num_training_complex:]\n",
    "complex_validation = remaining_complex[:num_validation_complex]\n",
    "complex_test = remaining_complex[num_validation_complex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7c7ffb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split `simple` items across datasets with adjustable ratios\n",
    "training_ratio = 0.6  \n",
    "validation_ratio = 0.20  \n",
    "test_ratio = 0.20  \n",
    "\n",
    "# Total number of `simple` entries\n",
    "num_simple = len(simple_keys)\n",
    "\n",
    "# Calculate how many `simple` items go into each dataset\n",
    "num_training_simple = int(num_simple * training_ratio)\n",
    "num_validation_simple = int(num_simple * validation_ratio)\n",
    "num_test_simple = num_simple - num_training_simple - num_validation_simple  # Remaining for test\n",
    "\n",
    "# Split `simple` items\n",
    "simple_training = simple_keys[:num_training_simple]\n",
    "remaining_simple = simple_keys[num_training_simple:]\n",
    "validation_simple = remaining_simple[:num_validation_simple]\n",
    "test_simple = remaining_simple[num_validation_simple:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7e75d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Combine splits to form datasets\n",
    "training = {**{k: complex_items[k] for k in complex_training},\n",
    "            **{k: simple_items[k] for k in simple_training}}\n",
    "validation = {**{k: complex_items[k] for k in complex_validation},\n",
    "              **{k: simple_items[k] for k in validation_simple}}\n",
    "test = {**{k: complex_items[k] for k in complex_test},\n",
    "        **{k: simple_items[k] for k in test_simple}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d5270484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 7850 entries\n",
      "Validation dataset: 2616 entries\n",
      "Test dataset: 2618 entries\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Output statistics and save datasets\n",
    "print(f\"Training dataset: {len(training)} entries\")\n",
    "print(f\"Validation dataset: {len(validation)} entries\")\n",
    "print(f\"Test dataset: {len(test)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c37f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between training and validation: set()\n",
      "Overlap between training and test: set()\n",
      "Overlap between validation and test: set()\n"
     ]
    }
   ],
   "source": [
    "# verify overlapping between dataset (we want none)\n",
    "training_keys = set(training.keys())\n",
    "validation_keys = set(validation.keys())\n",
    "test_keys = set(test.keys())\n",
    "\n",
    "print(f\"Overlap between training and validation: {training_keys & validation_keys}\")\n",
    "print(f\"Overlap between training and test: {training_keys & test_keys}\")\n",
    "print(f\"Overlap between validation and test: {validation_keys & test_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a20d7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex in training: 860\n",
      "Complex in validation: 286\n",
      "Complex in test: 288\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of complex and simple elements between datasets\n",
    "training_complex = sum(1 for v in training.values() if v.get(\"complexity\") == \"complex\")\n",
    "validation_complex = sum(1 for v in validation.values() if v.get(\"complexity\") == \"complex\")\n",
    "test_complex = sum(1 for v in test.values() if v.get(\"complexity\") == \"complex\")\n",
    "\n",
    "print(f\"Complex in training: {training_complex}\")\n",
    "print(f\"Complex in validation: {validation_complex}\")\n",
    "print(f\"Complex in test: {test_complex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7c3a8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save datasets to JSON files with complexity\n",
    "with open(\"data/fine_tuning_datasets/training_with_complexity.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(training, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/validation_with_complexity.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(validation, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/test_with_complexity.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac9c47bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save datasets to JSON files WITHOUT complexity\n",
    "\n",
    "def remove_complexity_field(dataset):\n",
    "    return {key: {k: v for k, v in value.items() if k != \"complexity\"} for key, value in dataset.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4cc08930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset without complexity\n",
    "\n",
    "training_no_complexity = remove_complexity_field(training)\n",
    "validation_no_complexity = remove_complexity_field(validation)\n",
    "test_no_complexity = remove_complexity_field(test)\n",
    "\n",
    "with open(\"data/fine_tuning_datasets/training.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(training_no_complexity, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/validation.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(validation_no_complexity, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/test.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test_no_complexity, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5be3678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking simple and complex repartitions in the datasets\n",
    "\n",
    "# step 1 Load the JSON files\n",
    "file_path = \"data/fine_tuning_datasets/training_small_with_complexity.json\"  # Replace with your JSON file path\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "74f8096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Entries: 200\n",
      "Complex: 72 (36.00%)\n",
      "Simple: 128 (64.00%)\n"
     ]
    }
   ],
   "source": [
    "# Extract complexities\n",
    "complexities = [item['complexity'] for item in data.values()]\n",
    "\n",
    "# Count occurrences of each complexity type\n",
    "total = len(complexities)\n",
    "complex_count = complexities.count(\"complex\")\n",
    "simple_count = complexities.count(\"simple\")\n",
    "\n",
    "# Calculate percentages\n",
    "complex_percentage = (complex_count / total) * 100\n",
    "simple_percentage = (simple_count / total) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Total Entries: {total}\")\n",
    "print(f\"Complex: {complex_count} ({complex_percentage:.2f}%)\")\n",
    "print(f\"Simple: {simple_count} ({simple_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff08d25",
   "metadata": {},
   "source": [
    "#### Dataset 200 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7a2a7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many lines for `complex` and `simple` in each small dataset\n",
    "training_complex_count = int(TOTAL_LINES_SMALL * TRAINING_RATIO * complex_ratio_training)\n",
    "validation_complex_count = int(TOTAL_LINES_SMALL * VALIDATION_RATIO * complex_ratio_validation)\n",
    "test_complex_count = int(TOTAL_LINES_SMALL * TEST_RATIO * complex_ratio_test)\n",
    "\n",
    "training_simple_count = TOTAL_LINES_SMALL - training_complex_count\n",
    "validation_simple_count = TOTAL_LINES_SMALL - validation_complex_count\n",
    "test_simple_count = TOTAL_LINES_SMALL - test_complex_count\n",
    "\n",
    "# Step 1: Shuffle the `complex` and `simple` keys for random selection\n",
    "random.shuffle(complex_keys)\n",
    "random.shuffle(simple_keys)\n",
    "\n",
    "# Step 2: Select `complex` entries for small datasets\n",
    "training_complex_small = complex_keys[:training_complex_count]\n",
    "remaining_complex_small = complex_keys[training_complex_count:]\n",
    "validation_complex_small = remaining_complex_small[:validation_complex_count]\n",
    "test_complex_small = remaining_complex_small[validation_complex_count:validation_complex_count + test_complex_count]\n",
    "\n",
    "# Step 3: Select `simple` entries for small datasets\n",
    "training_simple_small = simple_keys[:training_simple_count]\n",
    "remaining_simple_small = simple_keys[training_simple_count:]\n",
    "validation_simple_small = remaining_simple_small[:validation_simple_count]\n",
    "test_simple_small = remaining_simple_small[validation_simple_count:validation_simple_count + test_simple_count]\n",
    "\n",
    "# Step 4: Combine `complex` and `simple` entries for small datasets\n",
    "training_small = {**{k: complex_items[k] for k in training_complex_small},\n",
    "                  **{k: simple_items[k] for k in training_simple_small}}\n",
    "validation_small = {**{k: complex_items[k] for k in validation_complex_small},\n",
    "                    **{k: simple_items[k] for k in validation_simple_small}}\n",
    "test_small = {**{k: complex_items[k] for k in test_complex_small},\n",
    "              **{k: simple_items[k] for k in test_simple_small}}\n",
    "\n",
    "# Step 5: Verify and save the small datasets\n",
    "\n",
    "training_small_no_complexity = remove_complexity_field(training_small)\n",
    "validation_small_no_complexity = remove_complexity_field(validation_small)\n",
    "test_small_no_complexity = remove_complexity_field(test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4eb5ee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 200 entries\n",
      "Validation dataset: 200 entries\n",
      "Test dataset: 200 entries\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset: {len(training_small)} entries\")\n",
    "print(f\"Validation dataset: {len(validation_small)} entries\")\n",
    "print(f\"Test dataset: {len(test_small)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "46e11eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the small datasets to JSON files (with complexity)\n",
    "with open(\"data/fine_tuning_datasets/training_small_with_complexity.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(training_small, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/validation_small_with_complexity.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(validation_small, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/test_small_with_complexity.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test_small, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fb831bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the small datasets to JSON files\n",
    "with open(\"data/fine_tuning_datasets/training_small.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(training_small_no_complexity, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/validation_small.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(validation_small_no_complexity, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/test_small.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test_small_no_complexity, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041d371",
   "metadata": {},
   "source": [
    "#### Dataset 600 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "84eaea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many lines for `complex` and `simple` in each medium dataset\n",
    "training_complex_count = int(TOTAL_LINES_MEDIUM * TRAINING_RATIO * complex_ratio_training)\n",
    "validation_complex_count = int(TOTAL_LINES_MEDIUM * VALIDATION_RATIO * complex_ratio_validation)\n",
    "test_complex_count = int(TOTAL_LINES_MEDIUM * TEST_RATIO * complex_ratio_test)\n",
    "\n",
    "training_simple_count = TOTAL_LINES_MEDIUM - training_complex_count\n",
    "validation_simple_count = TOTAL_LINES_MEDIUM - validation_complex_count\n",
    "test_simple_count = TOTAL_LINES_MEDIUM - test_complex_count\n",
    "\n",
    "# Step 1: Shuffle the `complex` and `simple` keys for random selection\n",
    "random.shuffle(complex_keys)\n",
    "random.shuffle(simple_keys)\n",
    "\n",
    "# Step 2: Select `complex` entries for medium datasets\n",
    "training_complex_medium = complex_keys[:training_complex_count]\n",
    "remaining_complex_medium = complex_keys[training_complex_count:]\n",
    "validation_complex_medium = remaining_complex_medium[:validation_complex_count]\n",
    "test_complex_medium = remaining_complex_medium[validation_complex_count:validation_complex_count + test_complex_count]\n",
    "\n",
    "# Step 3: Select `simple` entries for medium datasets\n",
    "training_simple_medium = simple_keys[:training_simple_count]\n",
    "remaining_simple_medium = simple_keys[training_simple_count:]\n",
    "validation_simple_medium = remaining_simple_medium[:validation_simple_count]\n",
    "test_simple_medium = remaining_simple_medium[validation_simple_count:validation_simple_count + test_simple_count]\n",
    "\n",
    "# Step 4: Combine `complex` and `simple` entries for medium datasets\n",
    "training_medium = {**{k: complex_items[k] for k in training_complex_medium},\n",
    "                  **{k: simple_items[k] for k in training_simple_medium}}\n",
    "validation_medium = {**{k: complex_items[k] for k in validation_complex_medium},\n",
    "                    **{k: simple_items[k] for k in validation_simple_medium}}\n",
    "test_medium = {**{k: complex_items[k] for k in test_complex_medium},\n",
    "              **{k: simple_items[k] for k in test_simple_medium}}\n",
    "\n",
    "# Step 5: Verify and save the medium datasets\n",
    "\n",
    "training_medium_no_complexity = remove_complexity_field(training_medium)\n",
    "validation_medium_no_complexity = remove_complexity_field(validation_medium)\n",
    "test_medium_no_complexity = remove_complexity_field(test_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bfd2dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 600 entries\n",
      "Validation dataset: 600 entries\n",
      "Test dataset: 600 entries\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset: {len(training_medium)} entries\")\n",
    "print(f\"Validation dataset: {len(validation_medium)} entries\")\n",
    "print(f\"Test dataset: {len(test_medium)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0b0ee8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the medium datasets to JSON files (with complexity)\n",
    "with open(\"data/fine_tuning_datasets/training_medium_with_complexity.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(training_medium, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/validation_medium_with_complexity.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(validation_medium, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/test_medium_with_complexity.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test_medium, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "852600c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the medium datasets to JSON files\n",
    "with open(\"data/fine_tuning_datasets/training_medium.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(training_medium_no_complexity, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/validation_medium.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(validation_medium_no_complexity, f, ensure_ascii=False, indent=4)\n",
    "with open(\"data/fine_tuning_datasets/test_medium.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test_medium_no_complexity, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fcc88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
