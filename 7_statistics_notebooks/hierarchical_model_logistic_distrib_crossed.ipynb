{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef9f63-0999-44a4-b83b-81d90f7c1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Running on PyMC v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c740f3-68e2-4478-bee7-5a87398dfc8f",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad46c85-2c8e-495b-b4dd-ac03bcecc14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/playground_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9674e6c-c7ff-44d8-ac75-0e459b69bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.read_csv(data_path).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecaee4b-1b35-494a-b2a6-a3177c2bddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0c2db-b560-411e-9835-bf3eb633d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate covariates; we keep the ones that exist in df\n",
    "candidate_covars = [\"cmpl_complex\", \"cmpl_anim\", \"cmpl_det\", \"cmpl_def\", \"cmpl_indiv\", \"motion_type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961d158-e891-4c05-a1bb-bbd8558f8668",
   "metadata": {},
   "source": [
    "# Encoding important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd1b03-aee0-4c9a-bbab-c420a5e44152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure key columns exist\n",
    "for col in [\"cmpl_constr\", \"book_scroll\", \"lex\"]:\n",
    "    assert col in df_model.columns, f\"Missing column: {col}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b486826-9585-4b15-9b87-3fdafe471d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode outcome variable as categorical indices (pandas type) (no special reference; sum-to-zero)\n",
    "df_model[\"cmpl_constr\"] = df_model[\"cmpl_constr\"].astype(\"category\")\n",
    "class_labels = df_model[\"cmpl_constr\"].cat.categories.tolist()\n",
    "K = len(class_labels)\n",
    "y = df_model[\"cmpl_constr\"].cat.codes.values  # 0..K-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b782a0-f645-4c1c-a3fd-8623ab1059fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode grouping factors as categorical indices\n",
    "df_model[\"book_scroll\"] = df_model[\"book_scroll\"].astype(\"category\")\n",
    "df_model[\"lex\"] = df_model[\"lex\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad1dda-91bc-4c92-b129-d1e6777ab780",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_labels = df_model[\"book_scroll\"].cat.categories.tolist()\n",
    "verb_labels = df_model[\"lex\"].cat.categories.tolist()\n",
    "B = len(book_labels)\n",
    "V = len(verb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cacd5a0-8070-4397-9c8a-e760ec264f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_idx = df_model[\"book_scroll\"].cat.codes.values.astype(\"int32\")\n",
    "verb_idx = df_model[\"lex\"].cat.codes.values.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057dd3fe-7ac0-474e-bcfb-9ac52a0649c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build design matrix X for covariates\n",
    "have_covars = [c for c in candidate_covars if c in df_model.columns]\n",
    "X_df_parts = []\n",
    "\n",
    "for c in have_covars:\n",
    "    s = df_model[c]\n",
    "    # treat as categorical/boolean\n",
    "    s = s.astype(\"category\")\n",
    "    dummies = pd.get_dummies(s, prefix=c, drop_first=True)  # baseline within this covariate\n",
    "    if dummies.shape[1] > 0:\n",
    "        X_df_parts.append(dummies.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32a698-73e1-49d6-a8cc-9ef27e734091",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_df_parts) > 0:\n",
    "    X_df = pd.concat(X_df_parts, axis=1)\n",
    "    # Ensure no all-NA or all-constant columns slipped in\n",
    "    X_df = X_df.loc[:, X_df.apply(lambda col: col.nunique(dropna=True) > 1)]\n",
    "else:\n",
    "    # If no covariates present, create a zero-column matrix safely\n",
    "    X_df = pd.DataFrame(index=df_model.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ee2cf-fb50-43a2-b86f-9d3cddf906b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_df.columns.tolist()\n",
    "P = len(feature_names)\n",
    "\n",
    "X = X_df.to_numpy(dtype=float)\n",
    "\n",
    "N = len(df_model)\n",
    "\n",
    "print(f\"N={N}, K={K} classes, B={B} book_scroll levels, V={V} verb levels, P={P} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6128ab-8c2e-4b70-9e20-45c77d267408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2) Build & fit the PyMC model (sum-to-zero logits) =====\n",
    "coords = {\n",
    "    \"obs\": np.arange(N),\n",
    "    \"class\": class_labels,\n",
    "    \"book\": book_labels,\n",
    "    \"verb\": verb_labels,\n",
    "    \"feature\": feature_names,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092683b-374b-4ff0-a4f0-4e3d1659faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as model:\n",
    "    # Data containers (easy swapping for PPC / newdata later)\n",
    "    y_data       = pm.Data(\"y_data\", y, dims=\"obs\")\n",
    "    book_idx_dat = pm.Data(\"book_idx\", book_idx, dims=\"obs\")\n",
    "    verb_idx_dat = pm.Data(\"verb_idx\", verb_idx, dims=\"obs\")\n",
    "    X_dat        = pm.Data(\"X\", X, dims=(\"obs\",\"feature\")) if P > 0 else None\n",
    "\n",
    "    # Intercepts for ALL K classes (no reference class)\n",
    "    alpha = pm.Logistic(\"alpha\", 0.0, 2.5, dims=\"class\")\n",
    "\n",
    "    # Fixed effects per class (shape: P x K)\n",
    "    beta = pm.Logistic(\"beta\", 0.0, 1.5, dims=(\"feature\",\"class\"))\n",
    "\n",
    "    # Random intercepts: book_scroll (non-centered), per class ==> effect of the book\n",
    "    sigma_book = pm.HalfNormal(\"sigma_book\", 1.0, dims=\"class\") # how books impact classes (cmpl_constr)\n",
    "    z_book = pm.Logistic(\"z_book\", 0.0, 1.0, dims=(\"class\",\"book\")) # how each book impacts classes (cmpl_constr)\n",
    "    book_eff = pm.Deterministic(\"book_eff\", z_book * sigma_book[:, None], dims=(\"class\",\"book\"))  # K x B\n",
    "    \n",
    "    # sigma_book attenuate the effect of z_book; if, for one class, books have no impact or little impact \n",
    "    # but one particular book could have an impact\n",
    "    # the effect of the book in particular is attenuated\n",
    "    # it attenuates the effect of a specific book that could be random or due to another cause\n",
    "    # book_eff is the effect of each book on the classes (cmpl_constr)\n",
    "\n",
    "    # Random intercepts: verb (non-centered), per class ==> effect of the verb\n",
    "    sigma_verb = pm.HalfNormal(\"sigma_verb\", 1.0, dims=\"class\")\n",
    "    z_verb     = pm.Logistic(\"z_verb\", 0.0, 1.0, dims=(\"class\",\"verb\"))\n",
    "    verb_eff   = pm.Deterministic(\"verb_eff\", z_verb * sigma_verb[:, None], dims=(\"class\",\"verb\"))  # K x V\n",
    "\n",
    "    # Linear predictor for all K classes\n",
    "    # shapes:\n",
    "    #   alpha          : (K,) -> (N,K) after broadcasting\n",
    "    #   book_eff.T[idx]: (N,K)\n",
    "    #   verb_eff.T[idx]: (N,K)\n",
    "    #   X @ beta       : (N,P)@(P,K) -> (N,K), or zeros if P==0\n",
    "\n",
    "    # This is where the crossed factor choice is implemented (contri and eta)\n",
    "    # the book_scrolls effect and the verb effect are included at the same time\n",
    "    # they don't depend on each other (e.g. the book does not change the verb)\n",
    "    # every observation is classified independently by both factors\n",
    "    \n",
    "    book_contrib = pt.transpose(book_eff)[book_idx_dat, :]   # (N, K)\n",
    "    verb_contrib = pt.transpose(verb_eff)[verb_idx_dat, :]   # (N, K)\n",
    "    fixed_contrib = pt.dot(X_dat, beta) if P > 0 else pt.zeros((N, K))\n",
    "\n",
    "    # book_contrib is the contribution o the book on each observation\n",
    "    # fixed_contrib is the effect of the features on each observation\n",
    "\n",
    "    # Add alpha (random intercept for the classes) to the contributions\n",
    "\n",
    "    eta = alpha[None, :] + book_contrib + verb_contrib + fixed_contrib  # (N, K)\n",
    "\n",
    "    # Sum-to-zero constraint across classes for each observation (identifiability)\n",
    "    eta_centered = eta - pt.mean(eta, axis=1, keepdims=True)  # (N, K)\n",
    "    # ==> more readable, has no impact on the results probabilities\n",
    "\n",
    "    # Softmax probabilities and likelihood\n",
    "    p = pm.math.softmax(eta_centered, axis=1)  # (N, K)\n",
    "    y_obs = pm.Categorical(\"y_obs\", p=p, observed=y_data, dims=\"obs\")\n",
    "    # ==> transforms logits into probabilities\n",
    "\n",
    "    # Sampling - MCMC (NUTS: No-U-Turn Sampler, good for high-dimensional continuous parameters)\n",
    "    idata = pm.sample(\n",
    "        draws=1000, # how many kept samples per chain (after warmup); I reduced from 2000 to 1000 because of memory issues\n",
    "        tune=1000, # warmup/adaptation steps per chain (not kept); same reduction here\n",
    "        target_accept=0.9, # typical 0.8â€“0.95; higher reduces divergences\n",
    "        random_seed=2,\n",
    "        cores=None,  # let PyMC decide how many CPU cores to use in parallel\n",
    "    )\n",
    "\n",
    "    # Posterior predictive\n",
    "    ppc = pm.sample_posterior_predictive(idata, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504d7b9-4ba7-4d53-8fff-3a5281bb1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pooling magnitude\n",
    "az.plot_forest(idata, var_names=[\"sigma_book\",\"sigma_verb\"], combined=True)\n",
    "plt.title(\"Posterior distributions of group-level Standard Deviations - Model Logistic\")  # give the plot a title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d497c30-9811-46cf-9b07-03b80d055fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model fit\n",
    "y_sim = ppc.posterior_predictive[\"y_obs\"].stack(sample=(\"chain\",\"draw\")).values\n",
    "S, N = y_sim.shape\n",
    "K = len(class_labels)\n",
    "\n",
    "counts_obs = np.bincount(y, minlength=K)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot many spaghetti lines (no labels, to avoid clutter)\n",
    "for s in range(min(200, S)):\n",
    "    plt.plot(range(K), np.bincount(y_sim[s], minlength=K), color=\"skyblue\", alpha=0.2)\n",
    "\n",
    "# Plot a *single* invisible proxy line for the legend\n",
    "plt.plot([], [], color=\"skyblue\", alpha=0.5, label=\"Posterior predictive draws\")\n",
    "\n",
    "# Observed counts\n",
    "plt.plot(range(K), counts_obs, color=\"black\", lw=2, label=\"Observed\")\n",
    "\n",
    "plt.xticks(range(K), class_labels, rotation=45)\n",
    "plt.title(\"Global spaghetti PPC - Model with Logistic Distribution\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4fa25-5579-4058-a7b9-d6ba893a2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model fit with histogram for posterior draws\n",
    "S, N = y_sim.shape\n",
    "K = len(class_labels)\n",
    "counts_obs = np.bincount(y, minlength=K)\n",
    "\n",
    "# ---- Grid of panels: one histogram per class ----\n",
    "cols = min(5, K)                       # up to 5 per row; tweak as you like\n",
    "rows = int(np.ceil(K / cols))\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows), sharey=True)\n",
    "axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "for k in range(K):\n",
    "    sim_counts_k = (y_sim == k).sum(axis=1)         # (S,)\n",
    "    ax = axes[k]\n",
    "    ax.hist(sim_counts_k, bins=30, color=\"gray\", alpha=0.6)\n",
    "    ax.axvline(counts_obs[k], color=\"red\", linestyle=\"--\", lw=2, label=\"Observed\")\n",
    "    ax.set_title(class_labels[k])\n",
    "    ax.set_xlabel(\"Simulated counts\") # how many observations of class k are in a whole simulated dataset\n",
    "    if k % cols == 0:\n",
    "        ax.set_ylabel(\"Frequency across draws\") # how often did a simulated dataset produce that count\n",
    "    ax.legend()\n",
    "\n",
    "# Hide any empty subplots if K doesn't fill the grid\n",
    "for j in range(K, rows*cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313910c-9d5a-489a-83b1-57ece7ce1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = az.summary(idata, var_names=[\"alpha\",\"sigma_book\",\"sigma_verb\"], round_to=2)\n",
    "s\n",
    "# mean column: the posterior mean of the parameter, expected value under the posterior distribution\n",
    "# mean tells the average posterior estimate of the variation for the book or verb for each class\n",
    "\n",
    "# sigma_books: how much book_scrolls differ from each other \n",
    "# in their baseline preference for that outcome class\n",
    "\n",
    "# The next line shows only the rows where an effect is detected (0 present in the HDI)\n",
    "\n",
    "#s[((s[\"hdi_3%\"] < 0) & (s[\"hdi_97%\"] < 0)) | ((s[\"hdi_3%\"] > 0) & (s[\"hdi_97%\"] > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ed1a3-d851-446f-8f64-645bb46a1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = az.summary(idata, var_names=[\"verb_eff\"], round_to=2)\n",
    "s\n",
    "s[((s[\"hdi_3%\"] < 0) & (s[\"hdi_97%\"] < 0)) | ((s[\"hdi_3%\"] > 0) & (s[\"hdi_97%\"] > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e7fef-a7a7-4fdf-89da-2b8a676fe480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the effect of features (beta)\n",
    "\n",
    "s = az.summary(idata, var_names=[\"beta\"], round_to=2)\n",
    "s[((s[\"hdi_3%\"] < 0) & (s[\"hdi_97%\"] < 0)) | ((s[\"hdi_3%\"] > 0) & (s[\"hdi_97%\"] > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f3cc2-eb80-4ec5-b30d-d4559002e1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
