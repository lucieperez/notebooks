{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ef9f63-0999-44a4-b83b-81d90f7c1f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC v5.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Running on PyMC v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c740f3-68e2-4478-bee7-5a87398dfc8f",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad46c85-2c8e-495b-b4dd-ac03bcecc14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/playground_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9674e6c-c7ff-44d8-ac75-0e459b69bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.read_csv(data_path).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eecaee4b-1b35-494a-b2a6-a3177c2bddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d0c2db-b560-411e-9835-bf3eb633d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate covariates; we keep the ones that exist in df\n",
    "candidate_covars = [\"cmpl_complex\", \"cmpl_anim\", \"cmpl_det\", \"cmpl_def\", \"cmpl_indiv\", \"motion_type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961d158-e891-4c05-a1bb-bbd8558f8668",
   "metadata": {},
   "source": [
    "# Encoding important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ecd1b03-aee0-4c9a-bbab-c420a5e44152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure key columns exist\n",
    "for col in [\"cmpl_constr\", \"book_scroll\", \"lex\"]:\n",
    "    assert col in df_model.columns, f\"Missing column: {col}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b486826-9585-4b15-9b87-3fdafe471d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Encode outcome as categorical (no special reference; sum-to-zero)\n",
    "df_model[\"cmpl_constr\"] = df_model[\"cmpl_constr\"].astype(\"category\")\n",
    "class_labels = df_model[\"cmpl_constr\"].cat.categories.tolist()\n",
    "K = len(class_labels)\n",
    "y = df_model[\"cmpl_constr\"].cat.codes.values  # 0..K-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b782a0-f645-4c1c-a3fd-8623ab1059fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Encode grouping factors as categorical indices\n",
    "df_model[\"book_scroll\"] = df_model[\"book_scroll\"].astype(\"category\")\n",
    "df_model[\"lex\"] = df_model[\"lex\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ad1dda-91bc-4c92-b129-d1e6777ab780",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_labels = df_model[\"book_scroll\"].cat.categories.tolist()\n",
    "verb_labels = df_model[\"lex\"].cat.categories.tolist()\n",
    "B = len(book_labels)\n",
    "V = len(verb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cacd5a0-8070-4397-9c8a-e760ec264f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_idx = df_model[\"book_scroll\"].cat.codes.values.astype(\"int32\")\n",
    "verb_idx = df_model[\"lex\"].cat.codes.values.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "057dd3fe-7ac0-474e-bcfb-9ac52a0649c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build design matrix X for covariates\n",
    "have_covars = [c for c in candidate_covars if c in df_model.columns]\n",
    "X_df_parts = []\n",
    "\n",
    "for c in have_covars:\n",
    "    s = df_model[c]\n",
    "    # treat as categorical/boolean\n",
    "    s = s.astype(\"category\")\n",
    "    dummies = pd.get_dummies(s, prefix=c, drop_first=True)  # baseline within this covariate\n",
    "    if dummies.shape[1] > 0:\n",
    "        X_df_parts.append(dummies.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf32a698-73e1-49d6-a8cc-9ef27e734091",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_df_parts) > 0:\n",
    "    X_df = pd.concat(X_df_parts, axis=1)\n",
    "    # Ensure no all-NA or all-constant columns slipped in\n",
    "    X_df = X_df.loc[:, X_df.apply(lambda col: col.nunique(dropna=True) > 1)]\n",
    "else:\n",
    "    # If no covariates present, create a zero-column matrix safely\n",
    "    X_df = pd.DataFrame(index=df_model.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba8ee2cf-fb50-43a2-b86f-9d3cddf906b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=4109, K=4 classes, B=158 book_scroll levels, V=47 verb levels, P=13 features\n"
     ]
    }
   ],
   "source": [
    "feature_names = X_df.columns.tolist()\n",
    "P = len(feature_names)\n",
    "\n",
    "X = X_df.to_numpy(dtype=float)\n",
    "\n",
    "N = len(df_model)\n",
    "\n",
    "print(f\"N={N}, K={K} classes, B={B} book_scroll levels, V={V} verb levels, P={P} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db6128ab-8c2e-4b70-9e20-45c77d267408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2) Build & fit the PyMC model (sum-to-zero logits) =====\n",
    "coords = {\n",
    "    \"obs\": np.arange(N),\n",
    "    \"class\": class_labels,\n",
    "    \"book\": book_labels,\n",
    "    \"verb\": verb_labels,\n",
    "    \"feature\": feature_names,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e092683b-374b-4ff0-a4f0-4e3d1659faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "C:\\Users\\University\\.conda\\envs\\pymc_env\\Lib\\site-packages\\pytensor\\link\\c\\cmodule.py:2968: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.\n",
      "This usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.\n",
      "Alternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == 'NUMBA'` or passing `mode='NUMBA'` when compiling a PyTensor function.\n",
      "For more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library\n",
      "  warnings.warn(\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha, beta, sigma_book, z_book, sigma_verb, z_verb]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c403639ac60c4f05b405c02af55405d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 673 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Sampling: [y_obs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e116deba879d496fa3a3c9ca610d5630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pm.Model(coords=coords) as model:\n",
    "    # Data containers (easy swapping for PPC / newdata later)\n",
    "    y_data       = pm.Data(\"y_data\", y, dims=\"obs\")\n",
    "    book_idx_dat = pm.Data(\"book_idx\", book_idx, dims=\"obs\")\n",
    "    verb_idx_dat = pm.Data(\"verb_idx\", verb_idx, dims=\"obs\")\n",
    "    X_dat        = pm.Data(\"X\", X, dims=(\"obs\",\"feature\")) if P > 0 else None\n",
    "\n",
    "    # --- Intercepts for ALL K classes (no reference class)\n",
    "    alpha = pm.Normal(\"alpha\", 0.0, 2.5, dims=\"class\")\n",
    "\n",
    "    # --- Fixed effects per class (shape: P x K)\n",
    "    beta = pm.Normal(\"beta\", 0.0, 1.5, dims=(\"feature\",\"class\"))\n",
    "\n",
    "    # --- Random intercepts: book_scroll (non-centered), per class ==> effect of the book\n",
    "    sigma_book = pm.HalfNormal(\"sigma_book\", 1.0, dims=\"class\") # how books impact classes (cmpl_constr)\n",
    "    z_book = pm.Normal(\"z_book\", 0.0, 1.0, dims=(\"class\",\"book\")) # how each book impacts classes (cmpl_constr)\n",
    "    book_eff = pm.Deterministic(\"book_eff\", z_book * sigma_book[:, None], dims=(\"class\",\"book\"))  # K x B\n",
    "    \n",
    "    # sigma_book attenuate the effect of z_book; if, for one class, books have no impact or little impact \n",
    "    # but one particular book could have an impact\n",
    "    # the effect of the book in particular is attenuated\n",
    "    # it attenuates the effect of a specific book that could be random or due to another cause\n",
    "    # book_eff is the effect of each book on the classes (cmpl_constr)\n",
    "\n",
    "    # --- Random intercepts: verb (non-centered), per class ==> effect of the verb\n",
    "    sigma_verb = pm.HalfNormal(\"sigma_verb\", 1.0, dims=\"class\")\n",
    "    z_verb     = pm.Normal(\"z_verb\", 0.0, 1.0, dims=(\"class\",\"verb\"))\n",
    "    verb_eff   = pm.Deterministic(\"verb_eff\", z_verb * sigma_verb[:, None], dims=(\"class\",\"verb\"))  # K x V\n",
    "\n",
    "    # --- Linear predictor for all K classes\n",
    "    # shapes:\n",
    "    #   alpha          : (K,) -> (N,K) after broadcasting\n",
    "    #   book_eff.T[idx]: (N,K)\n",
    "    #   verb_eff.T[idx]: (N,K)\n",
    "    #   X @ beta       : (N,P)@(P,K) -> (N,K), or zeros if P==0\n",
    "\n",
    "    # This is where the crossed factor choice is implemented (contri and eta)\n",
    "    # the book_scrolls effect and the verb effect are included at the same time\n",
    "    # they don't depend on each other (e.g. the book does not change the verb)\n",
    "    # every observation is classified independently by both factors\n",
    "    \n",
    "    book_contrib = pt.transpose(book_eff)[book_idx_dat, :]   # (N, K)\n",
    "    verb_contrib = pt.transpose(verb_eff)[verb_idx_dat, :]   # (N, K)\n",
    "    fixed_contrib = pt.dot(X_dat, beta) if P > 0 else pt.zeros((N, K))\n",
    "\n",
    "    # book_contrib is the contribution o the book on each observation\n",
    "    # fixed_contrib is the effect of the features on each observation\n",
    "\n",
    "    # Add alpha (random intercept for the classes) to the contributions\n",
    "\n",
    "    eta = alpha[None, :] + book_contrib + verb_contrib + fixed_contrib  # (N, K)\n",
    "\n",
    "    # --- Sum-to-zero constraint across classes for each observation (identifiability)\n",
    "    eta_centered = eta - pt.mean(eta, axis=1, keepdims=True)  # (N, K)\n",
    "    # ==> more readable, has no impact on the results probabilities\n",
    "\n",
    "    # --- Softmax probabilities and likelihood\n",
    "    p = pm.math.softmax(eta_centered, axis=1)  # (N, K)\n",
    "    y_obs = pm.Categorical(\"y_obs\", p=p, observed=y_data, dims=\"obs\")\n",
    "    # ==> transforms logits into probabilities\n",
    "\n",
    "    # --- Sampling - MCMC (NUTS: No-U-Turn Sampler, good for high-dimensional continuous parameters)\n",
    "    idata = pm.sample(\n",
    "        draws=1000, # how many kept samples per chain (after warmup); I reduced from 2000 to 1000 because of memory issues\n",
    "        tune=1000, # warmup/adaptation steps per chain (not kept); same reduction here\n",
    "        chains=4, # number of independent chains\n",
    "        target_accept=0.9, # typical 0.8â€“0.95; higher reduces divergences\n",
    "        random_seed=42,\n",
    "        cores=None,  # let PyMC decide how many CPU cores to use in parallel\n",
    "    )\n",
    "\n",
    "    # Posterior predictive\n",
    "    ppc = pm.sample_posterior_predictive(idata, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504d7b9-4ba7-4d53-8fff-3a5281bb1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pooling magnitude\n",
    "az.plot_forest(idata, var_names=[\"sigma_book\",\"sigma_verb\"], combined=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf73d2-1b40-4a90-bca7-f47d210cc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive check (class frequencies)\n",
    "az.plot_ppc(ppc, num_pp_samples=200)\n",
    "\n",
    "plt.xticks(range(len(class_labels)), class_labels, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313910c-9d5a-489a-83b1-57ece7ce1923",
   "metadata": {},
   "outputs": [],
   "source": [
    " 3) Basic diagnostics & examples =====\n",
    "s = az.summary(idata, var_names=[\"alpha\",\"sigma_book\",\"sigma_verb\"], round_to=2)\n",
    "s\n",
    "# mean column: the posterior mean of the parameter, expected value under the posterior distribution\n",
    "# mean tells the average posterior estimate of the variation for the book or verb for each class\n",
    "\n",
    "# sigma_books: how much book_scrolls differ from each other \n",
    "# in their baseline preference for that outcome class\n",
    "\n",
    "# The next line shows only the rows where an effect is detected (0 present in the HDI)\n",
    "\n",
    "s[((s[\"hdi_3%\"] < 0) & (s[\"hdi_97%\"] < 0)) | ((s[\"hdi_3%\"] > 0) & (s[\"hdi_97%\"] > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ed1a3-d851-446f-8f64-645bb46a1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = az.summary(idata, var_names=[\"verb_eff\"], round_to=2)\n",
    "s\n",
    "#s[((s[\"hdi_3%\"] < 0) & (s[\"hdi_97%\"] < 0)) | ((s[\"hdi_3%\"] > 0) & (s[\"hdi_97%\"] > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e7fef-a7a7-4fdf-89da-2b8a676fe480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the effect of fefatures (beta)\n",
    "\n",
    "s = az.summary(idata, var_names=[\"beta\"], round_to=2)\n",
    "s[((s[\"hdi_3%\"] < 0) & (s[\"hdi_97%\"] < 0)) | ((s[\"hdi_3%\"] > 0) & (s[\"hdi_97%\"] > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f3cc2-eb80-4ec5-b30d-d4559002e1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
